{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JazzLSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennybobboy/ML1-5/blob/master/JazzLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMnuTEVbLf2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pygame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQa60ZpnD3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import IPython\n",
        "import sys\n",
        "from music21 import *\n",
        "import numpy as np\n",
        "#from grammar import *\n",
        "#from qa import *\n",
        "#from preprocess import * \n",
        "#from music_utils import *\n",
        "#from data_utils import *\n",
        "from keras.models import load_model, Model\n",
        "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDOMaxfJ2mPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/AdalbertoCq/Deep-Learning-Specialization-Coursera/blob/master/Sequence%20Models/week1/LSTM%20Network/qa.py\n",
        "from itertools import zip_longest\n",
        "import random\n",
        "\n",
        "from music21 import *\n",
        "\n",
        "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
        "\n",
        "''' Helper function to down num to the nearest multiple of mult. '''\n",
        "def __roundDown(num, mult):\n",
        "    return (float(num) - (float(num) % mult))\n",
        "\n",
        "''' Helper function to round up num to nearest multiple of mult. '''\n",
        "def __roundUp(num, mult):\n",
        "    return __roundDown(num, mult) + mult\n",
        "\n",
        "''' Helper function that, based on if upDown < 0 or upDown >= 0, rounds number \n",
        "    down or up respectively to nearest multiple of mult. '''\n",
        "def __roundUpDown(num, mult, upDown):\n",
        "    if upDown < 0:\n",
        "        return __roundDown(num, mult)\n",
        "    else:\n",
        "        return __roundUp(num, mult)\n",
        "\n",
        "''' Helper function, from recipes, to iterate over list in chunks of n \n",
        "    length. '''\n",
        "def __grouper(iterable, n, fillvalue=None):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args, fillvalue=fillvalue)\n",
        "\n",
        "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
        "\n",
        "''' Smooth the measure, ensuring that everything is in standard note lengths \n",
        "    (e.g., 0.125, 0.250, 0.333 ... ). '''\n",
        "def prune_grammar(curr_grammar):\n",
        "    pruned_grammar = curr_grammar.split(' ')\n",
        "\n",
        "    for ix, gram in enumerate(pruned_grammar):\n",
        "        terms = gram.split(',')\n",
        "        terms[1] = str(__roundUpDown(float(terms[1]), 0.250, \n",
        "            random.choice([-1, 1])))\n",
        "        pruned_grammar[ix] = ','.join(terms)\n",
        "    pruned_grammar = ' '.join(pruned_grammar)\n",
        "\n",
        "    return pruned_grammar\n",
        "\n",
        "''' Remove repeated notes, and notes that are too close together. '''\n",
        "def prune_notes(curr_notes):\n",
        "    for n1, n2 in __grouper(curr_notes, n=2):\n",
        "        if n2 == None: # corner case: odd-length list\n",
        "            continue\n",
        "        if isinstance(n1, note.Note) and isinstance(n2, note.Note):\n",
        "            if n1.nameWithOctave == n2.nameWithOctave:\n",
        "                curr_notes.remove(n2)\n",
        "\n",
        "    return curr_notes\n",
        "\n",
        "''' Perform quality assurance on notes '''\n",
        "def clean_up_notes(curr_notes):\n",
        "    removeIxs = []\n",
        "    for ix, m in enumerate(curr_notes):\n",
        "        # QA1: ensure nothing is of 0 quarter note len, if so changes its len\n",
        "        if (m.quarterLength == 0.0):\n",
        "            m.quarterLength = 0.250\n",
        "        # QA2: ensure no two melody notes have same offset, i.e. form a chord.\n",
        "        # Sorted, so same offset would be consecutive notes.\n",
        "        if (ix < (len(curr_notes) - 1)):\n",
        "            if (m.offset == curr_notes[ix + 1].offset and\n",
        "                isinstance(curr_notes[ix + 1], note.Note)):\n",
        "                removeIxs.append((ix + 1))\n",
        "    curr_notes = [i for ix, i in enumerate(curr_notes) if ix not in removeIxs]\n",
        "\n",
        "    return curr_notes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZLlTTuNCk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -NS \"https://github.com/enggen/Deep-Learning-Coursera/raw/master/Sequence%20Models/Week1/Jazz%20improvisation%20with%20LSTM/data/30s_seq.mp3\"\n",
        "!wget -NS \"https://github.com/jisungk/deepjazz/raw/master/midi/original_metheny.mid\"\n",
        "!wget -NS \"https://github.com/enggen/Deep-Learning-Coursera/raw/master/Sequence%20Models/Week1/Jazz%20improvisation%20with%20LSTM/data/30s_trained_model.mp3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3LlvARK3fjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict, defaultdict\n",
        "from itertools import groupby\n",
        "from music21 import *\n",
        "import copy, random, pdb\n",
        " \n",
        "''' Helper function to determine if a note is a scale tone. '''\n",
        "def __is_scale_tone(chord, note):\n",
        "    # Method: generate all scales that have the chord notes th check if note is\n",
        "    # in names\n",
        " \n",
        "    # Derive major or minor scales (minor if 'other') based on the quality\n",
        "    # of the chord.\n",
        "    scaleType = scale.DorianScale() # i.e. minor pentatonic\n",
        "    if chord.quality == 'major':\n",
        "        scaleType = scale.MajorScale()\n",
        "    # Can change later to deriveAll() for flexibility. If so then use list\n",
        "    # comprehension of form [x for a in b for x in a].\n",
        "    scales = scaleType.derive(chord) # use deriveAll() later for flexibility\n",
        "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
        "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
        " \n",
        "    # Get note name. Return true if in the list of note names.\n",
        "    noteName = note.name\n",
        "    return (noteName in allNoteNames)\n",
        " \n",
        "''' Helper function to determine if a note is an approach tone. '''\n",
        "def __is_approach_tone(chord, note):\n",
        "    # Method: see if note is +/- 1 a chord tone.\n",
        " \n",
        "    for chordPitch in chord.pitches:\n",
        "        stepUp = chordPitch.transpose(1)\n",
        "        stepDown = chordPitch.transpose(-1)\n",
        "        if (note.name == stepDown.name or\n",
        "            note.name == stepDown.getEnharmonic().name or\n",
        "            note.name == stepUp.name or\n",
        "            note.name == stepUp.getEnharmonic().name):\n",
        "                return True\n",
        "    return False\n",
        " \n",
        "''' Helper function to determine if a note is a chord tone. '''\n",
        "def __is_chord_tone(lastChord, note):\n",
        "    return (note.name in (p.name for p in lastChord.pitches))\n",
        " \n",
        "''' Helper function to generate a chord tone. '''\n",
        "def __generate_chord_tone(lastChord):\n",
        "    lastChordNoteNames = [p.nameWithOctave for p in lastChord.pitches]\n",
        "    return note.Note(random.choice(lastChordNoteNames))\n",
        " \n",
        "''' Helper function to generate a scale tone. '''\n",
        "def __generate_scale_tone(lastChord):\n",
        "    # Derive major or minor scales (minor if 'other') based on the quality\n",
        "    # of the lastChord.\n",
        "    scaleType = scale.WeightedHexatonicBlues() # minor pentatonic\n",
        "    if lastChord.quality == 'major':\n",
        "        scaleType = scale.MajorScale()\n",
        "    # Can change later to deriveAll() for flexibility. If so then use list\n",
        "    # comprehension of form [x for a in b for x in a].\n",
        "    scales = scaleType.derive(lastChord) # use deriveAll() later for flexibility\n",
        "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
        "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
        " \n",
        "    # Return a note (no octave here) in a scale that matches the lastChord.\n",
        "    sNoteName = random.choice(allNoteNames)\n",
        "    lastChordSort = lastChord.sortAscending()\n",
        "    sNoteOctave = random.choice([i.octave for i in lastChordSort.pitches])\n",
        "    sNote = note.Note((\"%s%s\" % (sNoteName, sNoteOctave)))\n",
        "    return sNote\n",
        " \n",
        "''' Helper function to generate an approach tone. '''\n",
        "def __generate_approach_tone(lastChord):\n",
        "    sNote = __generate_scale_tone(lastChord)\n",
        "    aNote = sNote.transpose(random.choice([1, -1]))\n",
        "    return aNote\n",
        " \n",
        "''' Helper function to generate a random tone. '''\n",
        "def __generate_arbitrary_tone(lastChord):\n",
        "    return __generate_scale_tone(lastChord) # fix later, make random note.\n",
        " \n",
        " \n",
        "''' Given the notes in a measure ('measure') and the chords in that measure\n",
        "    ('chords'), generate a list of abstract grammatical symbols to represent \n",
        "    that measure as described in GTK's \"Learning Jazz Grammars\" (2009). \n",
        " \n",
        "    Inputs: \n",
        "    1) \"measure\" : a stream.Voice object where each element is a\n",
        "        note.Note or note.Rest object.\n",
        " \n",
        "        >>> m1\n",
        "        <music21.stream.Voice 328482572>\n",
        "        >>> m1[0]\n",
        "        <music21.note.Rest rest>\n",
        "        >>> m1[1]\n",
        "        <music21.note.Note C>\n",
        " \n",
        "        Can have instruments and other elements, removes them here.\n",
        " \n",
        "    2) \"chords\" : a stream.Voice object where each element is a chord.Chord.\n",
        " \n",
        "        >>> c1\n",
        "        <music21.stream.Voice 328497548>\n",
        "        >>> c1[0]\n",
        "        <music21.chord.Chord E-4 G4 C4 B-3 G#2>\n",
        "        >>> c1[1]\n",
        "        <music21.chord.Chord B-3 F4 D4 A3>\n",
        " \n",
        "        Can have instruments and other elements, removes them here. \n",
        " \n",
        "    Outputs:\n",
        "    1) \"fullGrammar\" : a string that holds the abstract grammar for measure.\n",
        "        Format: \n",
        "        (Remember, these are DURATIONS not offsets!)\n",
        "        \"R,0.125\" : a rest element of  (1/32) length, or 1/8 quarter note. \n",
        "        \"C,0.125<M-2,m-6>\" : chord note of (1/32) length, generated\n",
        "                             anywhere from minor 6th down to major 2nd down.\n",
        "                             (interval <a,b> is not ordered). '''\n",
        "def parse_melody(fullMeasureNotes, fullMeasureChords):\n",
        "    # Remove extraneous elements.x\n",
        "    measure = copy.deepcopy(fullMeasureNotes)\n",
        "    chords = copy.deepcopy(fullMeasureChords)\n",
        "    measure.removeByNotOfClass([note.Note, note.Rest])\n",
        "    chords.removeByNotOfClass([chord.Chord])\n",
        " \n",
        "    # Information for the start of the measure.\n",
        "    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.\n",
        "    # 2) measureStartOffset: how long from the measure start to the first element.\n",
        "    measureStartTime = measure[0].offset - (measure[0].offset % 4)\n",
        "    measureStartOffset  = measure[0].offset - measureStartTime\n",
        " \n",
        "    # Iterate over the notes and rests in measure, finding the grammar for each\n",
        "    # note in the measure and adding an abstract grammatical string for it. \n",
        " \n",
        "    fullGrammar = \"\"\n",
        "    prevNote = None # Store previous note. Need for interval.\n",
        "    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.\n",
        "    for ix, nr in enumerate(measure):\n",
        "        # Get the last chord. If no last chord, then (assuming chords is of length\n",
        "        # >0) shift first chord in chords to the beginning of the measure.\n",
        "        try: \n",
        "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
        "        except IndexError:\n",
        "            chords[0].offset = measureStartTime\n",
        "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
        " \n",
        "        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.\n",
        "        # Dealing with solo notes here. If unexpected chord: still call 'C'.\n",
        "        elementType = ' '\n",
        "        # R: First, check if it's a rest. Clearly a rest --> only one possibility.\n",
        "        if isinstance(nr, note.Rest):\n",
        "            elementType = 'R'\n",
        "        # C: Next, check to see if note pitch is in the last chord.\n",
        "        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):\n",
        "            elementType = 'C'\n",
        "        # L: (Complement tone) Skip this for now.\n",
        "        # S: Check if it's a scale tone.\n",
        "        elif __is_scale_tone(lastChord, nr):\n",
        "            elementType = 'S'\n",
        "        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.\n",
        "        elif __is_approach_tone(lastChord, nr):\n",
        "            elementType = 'A'\n",
        "        # X: Otherwise, it's an arbitrary tone. Generate random note.\n",
        "        else:\n",
        "            elementType = 'X'\n",
        " \n",
        "        # SECOND, get the length for each element. e.g. 8th note = R8, but\n",
        "        # to simplify things you'll use the direct num, e.g. R,0.125\n",
        "        if (ix == (len(measure)-1)):\n",
        "            # formula for a in \"a - b\": start of measure (e.g. 476) + 4\n",
        "            diff = measureStartTime + 4.0 - nr.offset\n",
        "        else:\n",
        "            diff = measure[ix + 1].offset - nr.offset\n",
        " \n",
        "        # Combine into the note info.\n",
        "        noteInfo = \"%s,%.3f\" % (elementType, nr.quarterLength) # back to diff\n",
        " \n",
        "        # THIRD, get the deltas (max range up, max range down) based on where\n",
        "        # the previous note was, +- minor 3. Skip rests (don't affect deltas).\n",
        "        intervalInfo = \"\"\n",
        "        if isinstance(nr, note.Note):\n",
        "            numNonRests += 1\n",
        "            if numNonRests == 1:\n",
        "                prevNote = nr\n",
        "            else:\n",
        "                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)\n",
        "                noteDistUpper = interval.add([noteDist, \"m3\"])\n",
        "                noteDistLower = interval.subtract([noteDist, \"m3\"])\n",
        "                intervalInfo = \",<%s,%s>\" % (noteDistUpper.directedName, \n",
        "                    noteDistLower.directedName)\n",
        "                # print \"Upper, lower: %s, %s\" % (noteDistUpper,\n",
        "                #     noteDistLower)\n",
        "                # print \"Upper, lower dnames: %s, %s\" % (\n",
        "                #     noteDistUpper.directedName,\n",
        "                #     noteDistLower.directedName)\n",
        "                # print \"The interval: %s\" % (intervalInfo)\n",
        "                prevNote = nr\n",
        " \n",
        "        # Return. Do lazy evaluation for real-time performance.\n",
        "        grammarTerm = noteInfo + intervalInfo \n",
        "        fullGrammar += (grammarTerm + \" \")\n",
        " \n",
        "    return fullGrammar.rstrip()\n",
        " \n",
        "''' Given a grammar string and chords for a measure, returns measure notes. '''\n",
        "def unparse_grammar(m1_grammar, m1_chords):\n",
        "    m1_elements = stream.Voice()\n",
        "    currOffset = 0.0 # for recalculate last chord.\n",
        "    prevElement = None\n",
        "    for ix, grammarElement in enumerate(m1_grammar.split(' ')):\n",
        "        terms = grammarElement.split(',')\n",
        "        currOffset += float(terms[1]) # works just fine\n",
        " \n",
        "        # Case 1: it's a rest. Just append\n",
        "        if terms[0] == 'R':\n",
        "            rNote = note.Rest(quarterLength = float(terms[1]))\n",
        "            m1_elements.insert(currOffset, rNote)\n",
        "            continue\n",
        " \n",
        "        # Get the last chord first so you can find chord note, scale note, etc.\n",
        "        try: \n",
        "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
        "        except IndexError:\n",
        "            m1_chords[0].offset = 0.0\n",
        "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
        " \n",
        "        # Case: no < > (should just be the first note) so generate from range\n",
        "        # of lowest chord note to highest chord note (if not a chord note, else\n",
        "        # just generate one of the actual chord notes). \n",
        " \n",
        "        # Case #1: if no < > to indicate next note range. Usually this lack of < >\n",
        "        # is for the first note (no precedent), or for rests.\n",
        "        if (len(terms) == 2): # Case 1: if no < >.\n",
        "            insertNote = note.Note() # default is C\n",
        " \n",
        "            # Case C: chord note.\n",
        "            if terms[0] == 'C':\n",
        "                insertNote = __generate_chord_tone(lastChord)\n",
        " \n",
        "            # Case S: scale note.\n",
        "            elif terms[0] == 'S':\n",
        "                insertNote = __generate_scale_tone(lastChord)\n",
        " \n",
        "            # Case A: approach note.\n",
        "            # Handle both A and X notes here for now.\n",
        "            else:\n",
        "                insertNote = __generate_approach_tone(lastChord)\n",
        " \n",
        "            # Update the stream of generated notes\n",
        "            insertNote.quarterLength = float(terms[1])\n",
        "            if insertNote.octave < 4:\n",
        "                insertNote.octave = 4\n",
        "            m1_elements.insert(currOffset, insertNote)\n",
        "            prevElement = insertNote\n",
        " \n",
        "        # Case #2: if < > for the increment. Usually for notes after the first one.\n",
        "        else:\n",
        "            # Get lower, upper intervals and notes.\n",
        "            interval1 = interval.Interval(terms[2].replace(\"<\",''))\n",
        "            interval2 = interval.Interval(terms[3].replace(\">\",''))\n",
        "            if interval1.cents > interval2.cents:\n",
        "                upperInterval, lowerInterval = interval1, interval2\n",
        "            else:\n",
        "                upperInterval, lowerInterval = interval2, interval1\n",
        "            lowPitch = interval.transposePitch(prevElement.pitch, lowerInterval)\n",
        "            highPitch = interval.transposePitch(prevElement.pitch, upperInterval)\n",
        "            numNotes = int(highPitch.ps - lowPitch.ps + 1) # for range(s, e)\n",
        " \n",
        "            # Case C: chord note, must be within increment (terms[2]).\n",
        "            # First, transpose note with lowerInterval to get note that is\n",
        "            # the lower bound. Then iterate over, and find valid notes. Then\n",
        "            # choose randomly from those.\n",
        "             \n",
        "            if terms[0] == 'C':\n",
        "                relevantChordTones = []\n",
        "                for i in range(0, numNotes):\n",
        "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
        "                    if __is_chord_tone(lastChord, currNote):\n",
        "                        relevantChordTones.append(currNote)\n",
        "                if len(relevantChordTones) > 1:\n",
        "                    insertNote = random.choice([i for i in relevantChordTones\n",
        "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
        "                elif len(relevantChordTones) == 1:\n",
        "                    insertNote = relevantChordTones[0]\n",
        "                else: # if no choices, set to prev element +-1 whole step\n",
        "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
        "                if insertNote.octave < 3:\n",
        "                    insertNote.octave = 3\n",
        "                insertNote.quarterLength = float(terms[1])\n",
        "                m1_elements.insert(currOffset, insertNote)\n",
        " \n",
        "            # Case S: scale note, must be within increment.\n",
        "            elif terms[0] == 'S':\n",
        "                relevantScaleTones = []\n",
        "                for i in range(0, numNotes):\n",
        "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
        "                    if __is_scale_tone(lastChord, currNote):\n",
        "                        relevantScaleTones.append(currNote)\n",
        "                if len(relevantScaleTones) > 1:\n",
        "                    insertNote = random.choice([i for i in relevantScaleTones\n",
        "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
        "                elif len(relevantScaleTones) == 1:\n",
        "                    insertNote = relevantScaleTones[0]\n",
        "                else: # if no choices, set to prev element +-1 whole step\n",
        "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
        "                if insertNote.octave < 3:\n",
        "                    insertNote.octave = 3\n",
        "                insertNote.quarterLength = float(terms[1])\n",
        "                m1_elements.insert(currOffset, insertNote)\n",
        " \n",
        "            # Case A: approach tone, must be within increment.\n",
        "            # For now: handle both A and X cases.\n",
        "            else:\n",
        "                relevantApproachTones = []\n",
        "                for i in range(0, numNotes):\n",
        "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
        "                    if __is_approach_tone(lastChord, currNote):\n",
        "                        relevantApproachTones.append(currNote)\n",
        "                if len(relevantApproachTones) > 1:\n",
        "                    insertNote = random.choice([i for i in relevantApproachTones\n",
        "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
        "                elif len(relevantApproachTones) == 1:\n",
        "                    insertNote = relevantApproachTones[0]\n",
        "                else: # if no choices, set to prev element +-1 whole step\n",
        "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
        "                if insertNote.octave < 3:\n",
        "                    insertNote.octave = 3\n",
        "                insertNote.quarterLength = float(terms[1])\n",
        "                m1_elements.insert(currOffset, insertNote)\n",
        " \n",
        "            # update the previous element.\n",
        "            prevElement = insertNote\n",
        " \n",
        "    return m1_elements"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEiEVLM65BlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from music21 import *\n",
        "from collections import defaultdict, OrderedDict\n",
        "from itertools import groupby\n",
        "#from grammar import *\n",
        "\n",
        "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
        "\n",
        "''' Helper function to parse a MIDI file into its measures and chords '''\n",
        "def __parse_midi(data_fn):\n",
        "    # Parse the MIDI data for separate melody and accompaniment parts.\n",
        "    midi_data = converter.parse(data_fn)\n",
        "    # Get melody part, compress into single voice.\n",
        "    melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.\n",
        "    melody1, melody2 = melody_stream.getElementsByClass(stream.Voice)\n",
        "    for j in melody2:\n",
        "        melody1.insert(j.offset, j)\n",
        "    melody_voice = melody1\n",
        "\n",
        "    for i in melody_voice:\n",
        "        if i.quarterLength == 0.0:\n",
        "            i.quarterLength = 0.25\n",
        "\n",
        "    # Change key signature to adhere to comp_stream (1 sharp, mode = major).\n",
        "    # Also add Electric Guitar. \n",
        "    melody_voice.insert(0, instrument.ElectricGuitar())\n",
        "    melody_voice.insert(0, key.KeySignature(sharps=1))\n",
        "\n",
        "    # The accompaniment parts. Take only the best subset of parts from\n",
        "    # the original data. Maybe add more parts, hand-add valid instruments.\n",
        "    # Should add least add a string part (for sparse solos).\n",
        "    # Verified are good parts: 0, 1, 6, 7 '''\n",
        "    partIndices = [0, 1, 6, 7]\n",
        "    comp_stream = stream.Voice()\n",
        "    comp_stream.append([j.flat for i, j in enumerate(midi_data) \n",
        "        if i in partIndices])\n",
        "\n",
        "    # Full stream containing both the melody and the accompaniment. \n",
        "    # All parts are flattened. \n",
        "    full_stream = stream.Voice()\n",
        "    for i in range(len(comp_stream)):\n",
        "        full_stream.append(comp_stream[i])\n",
        "    full_stream.append(melody_voice)\n",
        "\n",
        "    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).\n",
        "    # Note that for different instruments (with stream.flat), you NEED to use\n",
        "    # stream.Part(), not stream.Voice().\n",
        "    # Accompanied solo is in range [478, 548)\n",
        "    solo_stream = stream.Voice()\n",
        "    for part in full_stream:\n",
        "        curr_part = stream.Part()\n",
        "        curr_part.append(part.getElementsByClass(instrument.Instrument))\n",
        "        curr_part.append(part.getElementsByClass(tempo.MetronomeMark))\n",
        "        curr_part.append(part.getElementsByClass(key.KeySignature))\n",
        "        curr_part.append(part.getElementsByClass(meter.TimeSignature))\n",
        "        curr_part.append(part.getElementsByOffset(476, 548, \n",
        "                                                  includeEndBoundary=True))\n",
        "        cp = curr_part.flat\n",
        "        solo_stream.insert(cp)\n",
        "\n",
        "    # Group by measure so you can classify. \n",
        "    # Note that measure 0 is for the time signature, metronome, etc. which have\n",
        "    # an offset of 0.0.\n",
        "    melody_stream = solo_stream[-1]\n",
        "    measures = OrderedDict()\n",
        "    offsetTuples = [(int(n.offset / 4), n) for n in melody_stream]\n",
        "    measureNum = 0 # for now, don't use real m. nums (119, 120)\n",
        "    for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
        "        measures[measureNum] = [n[1] for n in group]\n",
        "        measureNum += 1\n",
        "\n",
        "    # Get the stream of chords.\n",
        "    # offsetTuples_chords: group chords by measure number.\n",
        "    chordStream = solo_stream[0]\n",
        "    chordStream.removeByClass(note.Rest)\n",
        "    chordStream.removeByClass(note.Note)\n",
        "    offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream]\n",
        "\n",
        "    # Generate the chord structure. Use just track 1 (piano) since it is\n",
        "    # the only instrument that has chords. \n",
        "    # Group into 4s, just like before. \n",
        "    chords = OrderedDict()\n",
        "    measureNum = 0\n",
        "    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
        "        chords[measureNum] = [n[1] for n in group]\n",
        "        measureNum += 1\n",
        "\n",
        "    # Fix for the below problem.\n",
        "    #   1) Find out why len(measures) != len(chords).\n",
        "    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't\n",
        "    #           actually show up, while the accompaniment's beat 1 right after does.\n",
        "    #           Actually on second thought: melody/comp start on Ab, and resolve to\n",
        "    #           the same key (Ab) so could actually just cut out last measure to loop.\n",
        "    #           Decided: just cut out the last measure. \n",
        "    del chords[len(chords) - 1]\n",
        "    assert len(chords) == len(measures)\n",
        "\n",
        "    return measures, chords\n",
        "\n",
        "''' Helper function to get the grammatical data from given musical data. '''\n",
        "def __get_abstract_grammars(measures, chords):\n",
        "    # extract grammars\n",
        "    abstract_grammars = []\n",
        "    for ix in range(1, len(measures)):\n",
        "        m = stream.Voice()\n",
        "        for i in measures[ix]:\n",
        "            m.insert(i.offset, i)\n",
        "        c = stream.Voice()\n",
        "        for j in chords[ix]:\n",
        "            c.insert(j.offset, j)\n",
        "        parsed = parse_melody(m, c)\n",
        "        abstract_grammars.append(parsed)\n",
        "\n",
        "    return abstract_grammars\n",
        "\n",
        "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
        "\n",
        "''' Get musical data from a MIDI file '''\n",
        "def get_musical_data(data_fn):\n",
        "    measures, chords = __parse_midi(data_fn)\n",
        "    abstract_grammars = __get_abstract_grammars(measures, chords)\n",
        "\n",
        "    return chords, abstract_grammars\n",
        "\n",
        "''' Get corpus data from grammatical data '''\n",
        "def get_corpus_data(abstract_grammars):\n",
        "    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]\n",
        "    values = set(corpus)\n",
        "    val_indices = dict((v, i) for i, v in enumerate(values))\n",
        "    indices_val = dict((i, v) for i, v in enumerate(values))\n",
        "\n",
        "    return corpus, values, val_indices, indices_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpwC3Uj-y-VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/brunoklein99/deep-learning-notes/blob/master/music_utils.py\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.layers import RepeatVector\n",
        "import sys\n",
        "from music21 import *\n",
        "import numpy as np\n",
        "#from grammar import *\n",
        "#from preprocess import *\n",
        "#from qa import *\n",
        "\n",
        "\n",
        "def data_processing(corpus, values_indices, m = 60, Tx = 30):\n",
        "    # cut the corpus into semi-redundant sequences of Tx values\n",
        "    Tx = Tx \n",
        "    N_values = len(set(corpus))\n",
        "    np.random.seed(0)\n",
        "    X = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
        "    Y = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
        "    for i in range(m):\n",
        "#         for t in range(1, Tx):\n",
        "        random_idx = np.random.choice(len(corpus) - Tx)\n",
        "        corp_data = corpus[random_idx:(random_idx + Tx)]\n",
        "        for j in range(Tx):\n",
        "            idx = values_indices[corp_data[j]]\n",
        "            if j != 0:\n",
        "                X[i, j, idx] = 1\n",
        "                Y[i, j-1, idx] = 1\n",
        "    \n",
        "    Y = np.swapaxes(Y,0,1)\n",
        "    Y = Y.tolist()\n",
        "    return np.asarray(X), np.asarray(Y), N_values \n",
        "\n",
        "def next_value_processing(model, next_value, x, predict_and_sample, indices_values, abstract_grammars, duration, max_tries = 1000, temperature = 0.5):\n",
        "    \"\"\"\n",
        "    Helper function to fix the first value.\n",
        "    \n",
        "    Arguments:\n",
        "    next_value -- predicted and sampled value, index between 0 and 77\n",
        "    x -- numpy-array, one-hot encoding of next_value\n",
        "    predict_and_sample -- predict function\n",
        "    indices_values -- a python dictionary mapping indices (0-77) into their corresponding unique value (ex: A,0.250,< m2,P-4 >)\n",
        "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
        "    duration -- scalar, index of the loop in the parent function\n",
        "    max_tries -- Maximum numbers of time trying to fix the value\n",
        "    \n",
        "    Returns:\n",
        "    next_value -- process predicted value\n",
        "    \"\"\"\n",
        "\n",
        "    # fix first note: must not have < > and not be a rest\n",
        "    if (duration < 0.00001):\n",
        "        tries = 0\n",
        "        while (next_value.split(',')[0] == 'R' or \n",
        "            len(next_value.split(',')) != 2):\n",
        "            # give up after 1000 tries; random from input's first notes\n",
        "            if tries >= max_tries:\n",
        "                #print('Gave up on first note generation after', max_tries, 'tries')\n",
        "                # np.random is exclusive to high\n",
        "                rand = np.random.randint(0, len(abstract_grammars))\n",
        "                next_value = abstract_grammars[rand].split(' ')[0]\n",
        "            else:\n",
        "                next_value = predict_and_sample(model, x, indices_values, temperature)\n",
        "\n",
        "            tries += 1\n",
        "            \n",
        "    return next_value\n",
        "\n",
        "\n",
        "def sequence_to_matrix(sequence, values_indices):\n",
        "    \"\"\"\n",
        "    Convert a sequence (slice of the corpus) into a matrix (numpy) of one-hot vectors corresponding \n",
        "    to indices in values_indices\n",
        "    \n",
        "    Arguments:\n",
        "    sequence -- python list\n",
        "    \n",
        "    Returns:\n",
        "    x -- numpy-array of one-hot vectors \n",
        "    \"\"\"\n",
        "    sequence_len = len(sequence)\n",
        "    x = np.zeros((1, sequence_len, len(values_indices)))\n",
        "    for t, value in enumerate(sequence):\n",
        "        if (not value in values_indices): print(value)\n",
        "        x[0, t, values_indices[value]] = 1.\n",
        "    return x\n",
        "\n",
        "def one_hot(x):\n",
        "    x = K.argmax(x)\n",
        "    x = tf.one_hot(x, 78) \n",
        "    x = RepeatVector(1)(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClEHHsz15TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from music_utils import * \n",
        "#from preprocess import * \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "chords, abstract_grammars = get_musical_data('original_metheny.mid')\n",
        "corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
        "N_tones = len(set(corpus))\n",
        "n_a = 64\n",
        "x_initializer = np.zeros((1, 1, 78))\n",
        "a_initializer = np.zeros((1, n_a))\n",
        "c_initializer = np.zeros((1, n_a))\n",
        "\n",
        "def load_music_utils():\n",
        "    chords, abstract_grammars = get_musical_data('original_metheny.mid')\n",
        "    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
        "    N_tones = len(set(corpus))\n",
        "    X, Y, N_tones = data_processing(corpus, tones_indices, 60, 30)   \n",
        "    return (X, Y, N_tones, indices_tones)\n",
        "\n",
        "\n",
        "def generate_music(inference_model, corpus = corpus, abstract_grammars = abstract_grammars, tones = tones, tones_indices = tones_indices, indices_tones = indices_tones, T_y = 10, max_tries = 1000, diversity = 0.5):\n",
        "    \"\"\"\n",
        "    Generates music using a model trained to learn musical patterns of a jazz soloist. Creates an audio stream\n",
        "    to save the music and play it.\n",
        "    \n",
        "    Arguments:\n",
        "    model -- Keras model Instance, output of djmodel()\n",
        "    corpus -- musical corpus, list of 193 tones as strings (ex: 'C,0.333,<P1,d-5>')\n",
        "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
        "    tones -- set of unique tones, ex: 'A,0.250,<M2,d-4>' is one element of the set.\n",
        "    tones_indices -- a python dictionary mapping unique tone (ex: A,0.250,< m2,P-4 >) into their corresponding indices (0-77)\n",
        "    indices_tones -- a python dictionary mapping indices (0-77) into their corresponding unique tone (ex: A,0.250,< m2,P-4 >)\n",
        "    Tx -- integer, number of time-steps used at training time\n",
        "    temperature -- scalar value, defines how conservative/creative the model is when generating music\n",
        "    \n",
        "    Returns:\n",
        "    predicted_tones -- python list containing predicted tones\n",
        "    \"\"\"\n",
        "    \n",
        "    # set up audio stream\n",
        "    out_stream = stream.Stream()\n",
        "    \n",
        "    # Initialize chord variables\n",
        "    curr_offset = 0.0                                     # variable used to write sounds to the Stream.\n",
        "    num_chords = int(len(chords) / 3)                     # number of different set of chords\n",
        "    \n",
        "    print(\"Predicting new values for different set of chords.\")\n",
        "    # Loop over all 18 set of chords. At each iteration generate a sequence of tones\n",
        "    # and use the current chords to convert it into actual sounds \n",
        "    for i in range(1, num_chords):\n",
        "        \n",
        "        # Retrieve current chord from stream\n",
        "        curr_chords = stream.Voice()\n",
        "        \n",
        "        # Loop over the chords of the current set of chords\n",
        "        for j in chords[i]:\n",
        "            # Add chord to the current chords with the adequate offset, no need to understand this\n",
        "            curr_chords.insert((j.offset % 4), j)\n",
        "        \n",
        "        # Generate a sequence of tones using the model\n",
        "        _, indices = predict_and_sample(inference_model)\n",
        "        indices = list(indices.squeeze())\n",
        "        pred = [indices_tones[p] for p in indices]\n",
        "        \n",
        "        predicted_tones = 'C,0.25 '\n",
        "        for k in range(len(pred) - 1):\n",
        "            predicted_tones += pred[k] + ' ' \n",
        "        \n",
        "        predicted_tones +=  pred[-1]\n",
        "                \n",
        "        #### POST PROCESSING OF THE PREDICTED TONES ####\n",
        "        # We will consider \"A\" and \"X\" as \"C\" tones. It is a common choice.\n",
        "        predicted_tones = predicted_tones.replace(' A',' C').replace(' X',' C')\n",
        "\n",
        "        # Pruning #1: smoothing measure\n",
        "        predicted_tones = prune_grammar(predicted_tones)\n",
        "        \n",
        "        # Use predicted tones and current chords to generate sounds\n",
        "        sounds = unparse_grammar(predicted_tones, curr_chords)\n",
        "\n",
        "        # Pruning #2: removing repeated and too close together sounds\n",
        "        sounds = prune_notes(sounds)\n",
        "\n",
        "        # Quality assurance: clean up sounds\n",
        "        sounds = clean_up_notes(sounds)\n",
        "\n",
        "        # Print number of tones/notes in sounds\n",
        "        print('Generated %s sounds using the predicted values for the set of chords (\"%s\") and after pruning' % (len([k for k in sounds if isinstance(k, note.Note)]), i))\n",
        "        \n",
        "        # Insert sounds into the output stream\n",
        "        for m in sounds:\n",
        "            out_stream.insert(curr_offset + m.offset, m)\n",
        "        for mc in curr_chords:\n",
        "            out_stream.insert(curr_offset + mc.offset, mc)\n",
        "\n",
        "        curr_offset += 4.0\n",
        "        \n",
        "    # Initialize tempo of the output stream with 130 bit per minute\n",
        "    out_stream.insert(0.0, tempo.MetronomeMark(number=130))\n",
        "\n",
        "    # Save audio stream to fine\n",
        "    mf = midi.translate.streamToMidiFile(out_stream)\n",
        "    mf.open(\"my_music.midi\", 'wb')\n",
        "    mf.write()\n",
        "    print(\"Your generated music is saved in my_music.midi\")\n",
        "    mf.close()\n",
        "    \n",
        "    # Play the final stream through output (see 'play' lambda function above)\n",
        "    # play = lambda x: midi.realtime.StreamPlayer(x).play()\n",
        "    # play(out_stream)\n",
        "    \n",
        "    return out_stream\n",
        "\n",
        "\n",
        "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
        "                       c_initializer = c_initializer):\n",
        "    \"\"\"\n",
        "    Predicts the next value of values using the inference model.\n",
        "    \n",
        "    Arguments:\n",
        "    inference_model -- Keras model instance for inference time\n",
        "    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n",
        "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
        "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
        "    Ty -- length of the sequence you'd like to generate.\n",
        "    \n",
        "    Returns:\n",
        "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
        "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
        "    indices = np.argmax(pred, axis = -1)\n",
        "    results = to_categorical(indices, num_classes=78)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return results, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1toNWunSiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IPython.display.Audio('30s_seq.mp3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Gwa7kEnYoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y, n_values, indices_values = load_music_utils()\n",
        "print('shape of X:', X.shape)\n",
        "print('number of training examples:', X.shape[0])\n",
        "print('Tx (length of sequence):', X.shape[1])\n",
        "print('total # of unique values:', n_values)\n",
        "print('Shape of Y:', Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZKkbJbKncaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 64 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqq1ya0Uni4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reshapor = Reshape((1, 78))                        # Used in Step 2.B of djmodel(), below\n",
        "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
        "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaasijxXnoKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: djmodel\n",
        "\n",
        "def djmodel(Tx, n_a, n_values):\n",
        "    \"\"\"\n",
        "    Implement the model\n",
        "    \n",
        "    Arguments:\n",
        "    Tx -- length of the sequence in a corpus\n",
        "    n_a -- the number of activations used in our model\n",
        "    n_values -- number of unique values in the music data \n",
        "    \n",
        "    Returns:\n",
        "    model -- a keras model with the \n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input of your model with a shape \n",
        "    X = Input(shape=(Tx, n_values))\n",
        "    \n",
        "    # Define s0, initial hidden state for the decoder LSTM\n",
        "    a0 = Input(shape=(n_a,), name='a0')\n",
        "    c0 = Input(shape=(n_a,), name='c0')\n",
        "    a = a0\n",
        "    c = c0\n",
        "    \n",
        "    ### START CODE HERE ### \n",
        "    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)\n",
        "    outputs = []\n",
        "    \n",
        "    # Step 2: Loop\n",
        "    for t in range(Tx):\n",
        "        \n",
        "        # Step 2.A: select the \"t\"th time step vector from X. \n",
        "        x = Lambda(lambda x: X[:,t,:])(X)\n",
        "        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)\n",
        "        x = reshapor(x)\n",
        "        # Step 2.C: Perform one step of the LSTM_cell\n",
        "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
        "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
        "        out = densor(a)\n",
        "        # Step 2.E: add the output to \"outputs\"\n",
        "        outputs = outputs + [out]\n",
        "        \n",
        "    # Step 3: Create model instance\n",
        "    model = Model(inputs=[X,a0,c0],outputs=outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3g3-uP7nuNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = djmodel(Tx = 30 , n_a = 64, n_values = 78)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN45CUnNn1Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAz3pUGUn5pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = 60\n",
        "a0 = np.zeros((m, n_a))\n",
        "c0 = np.zeros((m, n_a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzbycudNn9xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit([X, a0, c0], list(Y), epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOA9hyW9oCOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: music_inference_model\n",
        "\n",
        "def music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100):\n",
        "    \"\"\"\n",
        "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
        "    \n",
        "    Arguments:\n",
        "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
        "    densor -- the trained \"densor\" from model(), Keras layer object\n",
        "    n_values -- integer, umber of unique values\n",
        "    n_a -- number of units in the LSTM_cell\n",
        "    Ty -- integer, number of time steps to generate\n",
        "    \n",
        "    Returns:\n",
        "    inference_model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input of your model with a shape \n",
        "    x0 = Input(shape=(1, n_values))\n",
        "    \n",
        "    # Define s0, initial hidden state for the decoder LSTM\n",
        "    a0 = Input(shape=(n_a,), name='a0')\n",
        "    c0 = Input(shape=(n_a,), name='c0')\n",
        "    a = a0\n",
        "    c = c0\n",
        "    x = x0\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
        "    outputs = []\n",
        "    \n",
        "    # Step 2: Loop over Ty and generate a value at every time step\n",
        "    for t in range(Ty):\n",
        "        \n",
        "        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n",
        "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
        "        \n",
        "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
        "        out = densor(a)\n",
        "\n",
        "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 78) (≈1 line)\n",
        "        outputs.append(out)\n",
        "        \n",
        "        # Step 2.D: Select the next value according to \"out\", and set \"x\" to be the one-hot representation of the\n",
        "        #           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided \n",
        "        #           the line of code you need to do this. \n",
        "        x = Lambda(one_hot)(out)\n",
        "        \n",
        "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
        "    inference_model = Model(inputs=[x0,a0,c0], outputs=outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return inference_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Q4BI0hoKFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QstNTXBfoO9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_initializer = np.zeros((1, 1, 78))\n",
        "a_initializer = np.zeros((1, n_a))\n",
        "c_initializer = np.zeros((1, n_a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EsZsPPHoTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: predict_and_sample\n",
        "\n",
        "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
        "                       c_initializer = c_initializer):\n",
        "    \"\"\"\n",
        "    Predicts the next value of values using the inference model.\n",
        "    \n",
        "    Arguments:\n",
        "    inference_model -- Keras model instance for inference time\n",
        "    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n",
        "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
        "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
        "    \n",
        "    Returns:\n",
        "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
        "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
        "    pred = inference_model.predict([x_initializer,a_initializer,c_initializer])\n",
        "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
        "    indices = np.argmax(pred, axis=2)\n",
        "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )\n",
        "    results = to_categorical(indices)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return results, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnE1K58xoYe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
        "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
        "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
        "print(\"list(indices[12:18]) =\", list(indices[12:18]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2peYjpqT1TrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C9nsaI-ofc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_stream = generate_music(inference_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwoQ9PGcooah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IPython.display.Audio('30s_trained_model.mp3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTBQRigFLCKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pygame\n",
        "import pygame.midi\n",
        "pygame.init()\n",
        "pygame.midi.init()\n",
        "\n",
        "print(pygame.__version__)\n",
        "\n",
        "pygame.midi.get_default_output_id()\n",
        "\n",
        "#pygame.display.list_modes()\n",
        "\n",
        "#pygame.mixer.init(44100, -16,2,2048)\n",
        "\n",
        "#pygame.mixer.music.load(\"my_music.midi\")\n",
        "#pygame.mixer.music.play()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cGtO2u_n_Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install fluidsynth\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "!pip install midi2audio\n",
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio\n",
        "\n",
        "FluidSynth(\"font.sf2\").midi_to_audio('my_music.midi', 'test.wav')\n",
        "Audio(\"test.wav\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkGfreKyo2e3",
        "colab_type": "text"
      },
      "source": [
        "References\n",
        "\n",
        "The ideas presented in this notebook came primarily from three computational music papers cited below. The implementation here also took significant inspiration and used many components from Ji-Sung Kim's github repository.\n",
        "\n",
        "    Ji-Sung Kim, 2016, deepjazz\n",
        "    Jon Gillick, Kevin Tang and Robert Keller, 2009. Learning Jazz Grammars\n",
        "    Robert Keller and David Morrison, 2007, A Grammatical Approach to Automatic Improvisation\n",
        "    François Pachet, 1999, Surprising Harmonies\n",
        "\n",
        "We're also grateful to François Germain for valuable feedback.\n"
      ]
    }
  ]
}